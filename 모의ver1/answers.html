<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <title>모의고사 정답 모음</title>
  <style>
    body { font-family: 'Segoe UI', 'Nanum Gothic', sans-serif; margin: 24px; background: #fefefe; color: #1f2d3d; }
    h1 { text-align: center; margin-bottom: 32px; }
    h2 { margin-top: 32px; color: #274472; }
    table { width: 100%; border-collapse: collapse; margin-top: 12px; font-size: 0.92rem; }
    th, td { border: 1px solid #d5dee9; padding: 8px 10px; vertical-align: top; }
    th { background: #f0f5fc; }
    td.answer { width: 120px; font-weight: 600; color: #1a4d8f; }
    td.explanation { color: #44556b; }
  </style>
</head>
<body>
  <h1>모의고사 정답 및 해설</h1>
  <p>각 문항의 정답과 핵심 해설을 정리했습니다. 서술형 문항은 포인트 위주 예시 답안을 제공합니다.</p>
  <h2>모의고사 1</h2>
  <table>
    <tr><th>번호</th><th>유형</th><th>정답</th><th>해설/채점 포인트</th></tr>
    <tr><td>01</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">range(1, n+1)으로 끝값을 포함하고 조건을 뒤에 적는다.</td></tr>
    <tr><td>02</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">슬라이싱은 0 기반 인덱스이므로 step-1부터 끝까지 반환한다.</td></tr>
    <tr><td>03</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">행렬곱 결과 shape는 (m, n) x (n, p) -&gt; (m, p)이다.</td></tr>
    <tr><td>04</td><td>Python 기초 / 객관식</td><td class="answer">C</td><td class="explanation">자기 내적은 각 원소 제곱의 합이다.</td></tr>
    <tr><td>05</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">arange는 stop을 포함하지 않으므로 총 원소 수는 stop/step이다.</td></tr>
    <tr><td>06</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">tuple과 str은 불변 자료형이다.</td></tr>
    <tr><td>07</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">enumerate(iterable)는 기본으로 0부터 인덱스를 부여한다.</td></tr>
    <tr><td>08</td><td>Python 기초 / 주관식</td><td class="answer">15</td><td class="explanation">등차수열 합 공식 n(n+1)/2.</td></tr>
    <tr><td>09</td><td>Python 기초 / 주관식</td><td class="answer">4</td><td class="explanation">총 원소 수를 행 수로 나누면 열 수가 된다.</td></tr>
    <tr><td>10</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">MSE는 제곱 오차 평균으로 이상치에 민감하다.</td></tr>
    <tr><td>11</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">가중치 3개와 편향 1개가 필요하다.</td></tr>
    <tr><td>12</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">발산 시에는 학습률을 낮추어 안정화한다.</td></tr>
    <tr><td>13</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">L2 패널티는 큰 가중치를 억제해 과적합을 줄인다.</td></tr>
    <tr><td>14</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관으로 계수 추정이 불안정해진다.</td></tr>
    <tr><td>15</td><td>선형 회귀 / 객관식</td><td class="answer">A</td><td class="explanation">R^2은 1에 가까울수록 좋은 설명력을 가진다.</td></tr>
    <tr><td>16</td><td>선형 회귀 / 객관식</td><td class="answer">C</td><td class="explanation">스케일을 맞추면 가중치 학습이 안정적이다.</td></tr>
    <tr><td>17</td><td>선형 회귀 / 주관식</td><td class="answer">10.8</td><td class="explanation">단일 샘플 MSE 경사는 2*(y_hat - y)*x.</td></tr>
    <tr><td>18</td><td>선형 회귀 / 주관식</td><td class="answer">0.8</td><td class="explanation">학습 비율은 1 - test_size이다.</td></tr>
    <tr><td>19</td><td>EDA / 객관식</td><td class="answer">A</td><td class="explanation">상관계수는 변수 간 선형 관계를 보여준다.</td></tr>
    <tr><td>20</td><td>EDA / 객관식</td><td class="answer">D</td><td class="explanation">박스플롯은 단일 변수 분포만 보여준다.</td></tr>
    <tr><td>21</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">bin 수는 분포 파악을 위한 설계 요소다.</td></tr>
    <tr><td>22</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">pairplot은 산점도 행렬과 변수별 분포를 제공한다.</td></tr>
    <tr><td>23</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관 변수 제거는 다중공선성을 완화한다.</td></tr>
    <tr><td>24</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">평균 대체는 분산을 줄여 데이터 변동성을 왜곡할 수 있다.</td></tr>
    <tr><td>25</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">jointplot은 산점도와 히스토그램을 결합해 보여준다.</td></tr>
    <tr><td>26</td><td>EDA / 주관식</td><td class="answer">0.027</td><td class="explanation">결측률은 missing / total로 계산한다.</td></tr>
    <tr><td>27</td><td>EDA / 주관식</td><td class="answer">1.833</td><td class="explanation">z = (x - mean) / std.</td></tr>
    <tr><td>28</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">ReLU는 양수 구간에서 기울기를 유지한다.</td></tr>
    <tr><td>29</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">완전연결층은 입력과 출력을 모두 연결하므로 가중치가 많다.</td></tr>
    <tr><td>30</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">체인룰을 사용해 기울기를 전파한다.</td></tr>
    <tr><td>31</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">softmax는 확률 분포를 반환한다.</td></tr>
    <tr><td>32</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">드롭아웃은 무작위 비활성화로 일반화 성능을 높인다.</td></tr>
    <tr><td>33</td><td>MLP / 객관식</td><td class="answer">D</td><td class="explanation">배치 정규화는 층 깊이를 줄이지 않는다.</td></tr>
    <tr><td>34</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">검증 손실이 나빠지면 학습을 중단한다.</td></tr>
    <tr><td>35</td><td>MLP / 주관식</td><td class="answer">0.2357</td><td class="explanation">He 초기화 표준편차는 sqrt(2/fan_in)이다.</td></tr>
    <tr><td>36</td><td>MLP / 주관식</td><td class="answer">108</td><td class="explanation">가중치는 input*hidden, 편향은 hidden이다.</td></tr>
    <tr><td>37</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">WordPiece는 빈도 기반 서브워드를 학습한다.</td></tr>
    <tr><td>38</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">큰 vocab은 메모리와 연산 비용을 증가시킨다.</td></tr>
    <tr><td>39</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">임베딩 차원은 표현력과 효율 사이에서 균형을 맞춘다.</td></tr>
    <tr><td>40</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">길어진 시퀀스에서 기울기가 0에 수렴한다.</td></tr>
    <tr><td>41</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">입력 게이트는 후보 상태를 얼마나 반영할지 결정한다.</td></tr>
    <tr><td>42</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">A</td><td class="explanation">패딩 토큰은 손실 계산에서 제외되도록 마스킹해야 한다.</td></tr>
    <tr><td>43</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">teacher forcing은 노출 편향을 유발할 수 있다.</td></tr>
    <tr><td>44</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">4</td><td class="explanation">필요한 패딩 수는 target_len - seq_len.</td></tr>
    <tr><td>45</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">1.164</td><td class="explanation">c_t = f_t * c_{t-1} + i_t * g_t.</td></tr>
    <tr><td>46</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 데이터 수집 및 검증 -&gt; 결측치/이상치 처리 -&gt; 특징 스케일링 -&gt; 훈련/검증 분할 -&gt; 선형 회귀 학습 -&gt; 평가 지표 보고 및 개선 방안 제시.</td><td class="explanation">각 단계의 목적과 실행 방안을 포함해야 한다.</td></tr>
    <tr><td>47</td><td>서술형 / 서술형</td><td class="answer">예시 답안: pairplot/heatmap 활용, 상관계수 기준 특징 선택, 이상치/분포 확인, 시각화별 해석.</td><td class="explanation">EDA의 목적과 도구 선택 근거를 제시해야 한다.</td></tr>
    <tr><td>48</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 입력 -&gt; 은닉층 2~3개(ReLU), 드롭아웃 0.20, 배치 정규화, 최적화 기법 선택 근거.</td><td class="explanation">모델 설계 선택을 데이터 특성과 연결해야 한다.</td></tr>
    <tr><td>49</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 서브워드 토크나이저 선택, 패딩/마스킹 처리, 임베딩 층, LSTM/GRU 구성, 정규화 및 검증 전략 제시.</td><td class="explanation">NLP 파이프라인의 단계별 설정을 구체화해야 한다.</td></tr>
    <tr><td>50</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 학습률/은닉 유닛 탐색, 교차검증, 데이터 증강, 조기 종료, 정규화 기법 비교 등.</td><td class="explanation">향상 전략을 근거와 함께 기술해야 한다.</td></tr>
  </table>

  <h2>모의고사 2</h2>
  <table>
    <tr><th>번호</th><th>유형</th><th>정답</th><th>해설/채점 포인트</th></tr>
    <tr><td>01</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">range(1, n+1)으로 끝값을 포함하고 조건을 뒤에 적는다.</td></tr>
    <tr><td>02</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">슬라이싱은 0 기반 인덱스이므로 step-1부터 끝까지 반환한다.</td></tr>
    <tr><td>03</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">행렬곱 결과 shape는 (m, n) x (n, p) -&gt; (m, p)이다.</td></tr>
    <tr><td>04</td><td>Python 기초 / 객관식</td><td class="answer">C</td><td class="explanation">자기 내적은 각 원소 제곱의 합이다.</td></tr>
    <tr><td>05</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">arange는 stop을 포함하지 않으므로 총 원소 수는 stop/step이다.</td></tr>
    <tr><td>06</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">tuple과 str은 불변 자료형이다.</td></tr>
    <tr><td>07</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">enumerate(iterable)는 기본으로 0부터 인덱스를 부여한다.</td></tr>
    <tr><td>08</td><td>Python 기초 / 주관식</td><td class="answer">21</td><td class="explanation">등차수열 합 공식 n(n+1)/2.</td></tr>
    <tr><td>09</td><td>Python 기초 / 주관식</td><td class="answer">4</td><td class="explanation">총 원소 수를 행 수로 나누면 열 수가 된다.</td></tr>
    <tr><td>10</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">MSE는 제곱 오차 평균으로 이상치에 민감하다.</td></tr>
    <tr><td>11</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">가중치 3개와 편향 1개가 필요하다.</td></tr>
    <tr><td>12</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">발산 시에는 학습률을 낮추어 안정화한다.</td></tr>
    <tr><td>13</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">L2 패널티는 큰 가중치를 억제해 과적합을 줄인다.</td></tr>
    <tr><td>14</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관으로 계수 추정이 불안정해진다.</td></tr>
    <tr><td>15</td><td>선형 회귀 / 객관식</td><td class="answer">A</td><td class="explanation">R^2은 1에 가까울수록 좋은 설명력을 가진다.</td></tr>
    <tr><td>16</td><td>선형 회귀 / 객관식</td><td class="answer">C</td><td class="explanation">스케일을 맞추면 가중치 학습이 안정적이다.</td></tr>
    <tr><td>17</td><td>선형 회귀 / 주관식</td><td class="answer">12.8</td><td class="explanation">단일 샘플 MSE 경사는 2*(y_hat - y)*x.</td></tr>
    <tr><td>18</td><td>선형 회귀 / 주관식</td><td class="answer">0.8</td><td class="explanation">학습 비율은 1 - test_size이다.</td></tr>
    <tr><td>19</td><td>EDA / 객관식</td><td class="answer">A</td><td class="explanation">상관계수는 변수 간 선형 관계를 보여준다.</td></tr>
    <tr><td>20</td><td>EDA / 객관식</td><td class="answer">D</td><td class="explanation">박스플롯은 단일 변수 분포만 보여준다.</td></tr>
    <tr><td>21</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">bin 수는 분포 파악을 위한 설계 요소다.</td></tr>
    <tr><td>22</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">pairplot은 산점도 행렬과 변수별 분포를 제공한다.</td></tr>
    <tr><td>23</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관 변수 제거는 다중공선성을 완화한다.</td></tr>
    <tr><td>24</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">평균 대체는 분산을 줄여 데이터 변동성을 왜곡할 수 있다.</td></tr>
    <tr><td>25</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">jointplot은 산점도와 히스토그램을 결합해 보여준다.</td></tr>
    <tr><td>26</td><td>EDA / 주관식</td><td class="answer">0.029</td><td class="explanation">결측률은 missing / total로 계산한다.</td></tr>
    <tr><td>27</td><td>EDA / 주관식</td><td class="answer">1.714</td><td class="explanation">z = (x - mean) / std.</td></tr>
    <tr><td>28</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">ReLU는 양수 구간에서 기울기를 유지한다.</td></tr>
    <tr><td>29</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">완전연결층은 입력과 출력을 모두 연결하므로 가중치가 많다.</td></tr>
    <tr><td>30</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">체인룰을 사용해 기울기를 전파한다.</td></tr>
    <tr><td>31</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">softmax는 확률 분포를 반환한다.</td></tr>
    <tr><td>32</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">드롭아웃은 무작위 비활성화로 일반화 성능을 높인다.</td></tr>
    <tr><td>33</td><td>MLP / 객관식</td><td class="answer">D</td><td class="explanation">배치 정규화는 층 깊이를 줄이지 않는다.</td></tr>
    <tr><td>34</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">검증 손실이 나빠지면 학습을 중단한다.</td></tr>
    <tr><td>35</td><td>MLP / 주관식</td><td class="answer">0.2236</td><td class="explanation">He 초기화 표준편차는 sqrt(2/fan_in)이다.</td></tr>
    <tr><td>36</td><td>MLP / 주관식</td><td class="answer">130</td><td class="explanation">가중치는 input*hidden, 편향은 hidden이다.</td></tr>
    <tr><td>37</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">WordPiece는 빈도 기반 서브워드를 학습한다.</td></tr>
    <tr><td>38</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">큰 vocab은 메모리와 연산 비용을 증가시킨다.</td></tr>
    <tr><td>39</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">임베딩 차원은 표현력과 효율 사이에서 균형을 맞춘다.</td></tr>
    <tr><td>40</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">길어진 시퀀스에서 기울기가 0에 수렴한다.</td></tr>
    <tr><td>41</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">입력 게이트는 후보 상태를 얼마나 반영할지 결정한다.</td></tr>
    <tr><td>42</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">A</td><td class="explanation">패딩 토큰은 손실 계산에서 제외되도록 마스킹해야 한다.</td></tr>
    <tr><td>43</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">teacher forcing은 노출 편향을 유발할 수 있다.</td></tr>
    <tr><td>44</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">4</td><td class="explanation">필요한 패딩 수는 target_len - seq_len.</td></tr>
    <tr><td>45</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">1.492</td><td class="explanation">c_t = f_t * c_{t-1} + i_t * g_t.</td></tr>
    <tr><td>46</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 데이터 수집 및 검증 -&gt; 결측치/이상치 처리 -&gt; 특징 스케일링 -&gt; 훈련/검증 분할 -&gt; 선형 회귀 학습 -&gt; 평가 지표 보고 및 개선 방안 제시.</td><td class="explanation">각 단계의 목적과 실행 방안을 포함해야 한다.</td></tr>
    <tr><td>47</td><td>서술형 / 서술형</td><td class="answer">예시 답안: pairplot/heatmap 활용, 상관계수 기준 특징 선택, 이상치/분포 확인, 시각화별 해석.</td><td class="explanation">EDA의 목적과 도구 선택 근거를 제시해야 한다.</td></tr>
    <tr><td>48</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 입력 -&gt; 은닉층 2~3개(ReLU), 드롭아웃 0.25, 배치 정규화, 최적화 기법 선택 근거.</td><td class="explanation">모델 설계 선택을 데이터 특성과 연결해야 한다.</td></tr>
    <tr><td>49</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 서브워드 토크나이저 선택, 패딩/마스킹 처리, 임베딩 층, LSTM/GRU 구성, 정규화 및 검증 전략 제시.</td><td class="explanation">NLP 파이프라인의 단계별 설정을 구체화해야 한다.</td></tr>
    <tr><td>50</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 학습률/은닉 유닛 탐색, 교차검증, 데이터 증강, 조기 종료, 정규화 기법 비교 등.</td><td class="explanation">향상 전략을 근거와 함께 기술해야 한다.</td></tr>
  </table>

  <h2>모의고사 3</h2>
  <table>
    <tr><th>번호</th><th>유형</th><th>정답</th><th>해설/채점 포인트</th></tr>
    <tr><td>01</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">range(1, n+1)으로 끝값을 포함하고 조건을 뒤에 적는다.</td></tr>
    <tr><td>02</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">슬라이싱은 0 기반 인덱스이므로 step-1부터 끝까지 반환한다.</td></tr>
    <tr><td>03</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">행렬곱 결과 shape는 (m, n) x (n, p) -&gt; (m, p)이다.</td></tr>
    <tr><td>04</td><td>Python 기초 / 객관식</td><td class="answer">C</td><td class="explanation">자기 내적은 각 원소 제곱의 합이다.</td></tr>
    <tr><td>05</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">arange는 stop을 포함하지 않으므로 총 원소 수는 stop/step이다.</td></tr>
    <tr><td>06</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">tuple과 str은 불변 자료형이다.</td></tr>
    <tr><td>07</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">enumerate(iterable)는 기본으로 0부터 인덱스를 부여한다.</td></tr>
    <tr><td>08</td><td>Python 기초 / 주관식</td><td class="answer">28</td><td class="explanation">등차수열 합 공식 n(n+1)/2.</td></tr>
    <tr><td>09</td><td>Python 기초 / 주관식</td><td class="answer">4</td><td class="explanation">총 원소 수를 행 수로 나누면 열 수가 된다.</td></tr>
    <tr><td>10</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">MSE는 제곱 오차 평균으로 이상치에 민감하다.</td></tr>
    <tr><td>11</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">가중치 3개와 편향 1개가 필요하다.</td></tr>
    <tr><td>12</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">발산 시에는 학습률을 낮추어 안정화한다.</td></tr>
    <tr><td>13</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">L2 패널티는 큰 가중치를 억제해 과적합을 줄인다.</td></tr>
    <tr><td>14</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관으로 계수 추정이 불안정해진다.</td></tr>
    <tr><td>15</td><td>선형 회귀 / 객관식</td><td class="answer">A</td><td class="explanation">R^2은 1에 가까울수록 좋은 설명력을 가진다.</td></tr>
    <tr><td>16</td><td>선형 회귀 / 객관식</td><td class="answer">C</td><td class="explanation">스케일을 맞추면 가중치 학습이 안정적이다.</td></tr>
    <tr><td>17</td><td>선형 회귀 / 주관식</td><td class="answer">14.0</td><td class="explanation">단일 샘플 MSE 경사는 2*(y_hat - y)*x.</td></tr>
    <tr><td>18</td><td>선형 회귀 / 주관식</td><td class="answer">0.8</td><td class="explanation">학습 비율은 1 - test_size이다.</td></tr>
    <tr><td>19</td><td>EDA / 객관식</td><td class="answer">A</td><td class="explanation">상관계수는 변수 간 선형 관계를 보여준다.</td></tr>
    <tr><td>20</td><td>EDA / 객관식</td><td class="answer">D</td><td class="explanation">박스플롯은 단일 변수 분포만 보여준다.</td></tr>
    <tr><td>21</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">bin 수는 분포 파악을 위한 설계 요소다.</td></tr>
    <tr><td>22</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">pairplot은 산점도 행렬과 변수별 분포를 제공한다.</td></tr>
    <tr><td>23</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관 변수 제거는 다중공선성을 완화한다.</td></tr>
    <tr><td>24</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">평균 대체는 분산을 줄여 데이터 변동성을 왜곡할 수 있다.</td></tr>
    <tr><td>25</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">jointplot은 산점도와 히스토그램을 결합해 보여준다.</td></tr>
    <tr><td>26</td><td>EDA / 주관식</td><td class="answer">0.031</td><td class="explanation">결측률은 missing / total로 계산한다.</td></tr>
    <tr><td>27</td><td>EDA / 주관식</td><td class="answer">1.625</td><td class="explanation">z = (x - mean) / std.</td></tr>
    <tr><td>28</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">ReLU는 양수 구간에서 기울기를 유지한다.</td></tr>
    <tr><td>29</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">완전연결층은 입력과 출력을 모두 연결하므로 가중치가 많다.</td></tr>
    <tr><td>30</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">체인룰을 사용해 기울기를 전파한다.</td></tr>
    <tr><td>31</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">softmax는 확률 분포를 반환한다.</td></tr>
    <tr><td>32</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">드롭아웃은 무작위 비활성화로 일반화 성능을 높인다.</td></tr>
    <tr><td>33</td><td>MLP / 객관식</td><td class="answer">D</td><td class="explanation">배치 정규화는 층 깊이를 줄이지 않는다.</td></tr>
    <tr><td>34</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">검증 손실이 나빠지면 학습을 중단한다.</td></tr>
    <tr><td>35</td><td>MLP / 주관식</td><td class="answer">0.2132</td><td class="explanation">He 초기화 표준편차는 sqrt(2/fan_in)이다.</td></tr>
    <tr><td>36</td><td>MLP / 주관식</td><td class="answer">154</td><td class="explanation">가중치는 input*hidden, 편향은 hidden이다.</td></tr>
    <tr><td>37</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">WordPiece는 빈도 기반 서브워드를 학습한다.</td></tr>
    <tr><td>38</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">큰 vocab은 메모리와 연산 비용을 증가시킨다.</td></tr>
    <tr><td>39</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">임베딩 차원은 표현력과 효율 사이에서 균형을 맞춘다.</td></tr>
    <tr><td>40</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">길어진 시퀀스에서 기울기가 0에 수렴한다.</td></tr>
    <tr><td>41</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">입력 게이트는 후보 상태를 얼마나 반영할지 결정한다.</td></tr>
    <tr><td>42</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">A</td><td class="explanation">패딩 토큰은 손실 계산에서 제외되도록 마스킹해야 한다.</td></tr>
    <tr><td>43</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">teacher forcing은 노출 편향을 유발할 수 있다.</td></tr>
    <tr><td>44</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">4</td><td class="explanation">필요한 패딩 수는 target_len - seq_len.</td></tr>
    <tr><td>45</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">1.854</td><td class="explanation">c_t = f_t * c_{t-1} + i_t * g_t.</td></tr>
    <tr><td>46</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 데이터 수집 및 검증 -&gt; 결측치/이상치 처리 -&gt; 특징 스케일링 -&gt; 훈련/검증 분할 -&gt; 선형 회귀 학습 -&gt; 평가 지표 보고 및 개선 방안 제시.</td><td class="explanation">각 단계의 목적과 실행 방안을 포함해야 한다.</td></tr>
    <tr><td>47</td><td>서술형 / 서술형</td><td class="answer">예시 답안: pairplot/heatmap 활용, 상관계수 기준 특징 선택, 이상치/분포 확인, 시각화별 해석.</td><td class="explanation">EDA의 목적과 도구 선택 근거를 제시해야 한다.</td></tr>
    <tr><td>48</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 입력 -&gt; 은닉층 2~3개(ReLU), 드롭아웃 0.30, 배치 정규화, 최적화 기법 선택 근거.</td><td class="explanation">모델 설계 선택을 데이터 특성과 연결해야 한다.</td></tr>
    <tr><td>49</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 서브워드 토크나이저 선택, 패딩/마스킹 처리, 임베딩 층, LSTM/GRU 구성, 정규화 및 검증 전략 제시.</td><td class="explanation">NLP 파이프라인의 단계별 설정을 구체화해야 한다.</td></tr>
    <tr><td>50</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 학습률/은닉 유닛 탐색, 교차검증, 데이터 증강, 조기 종료, 정규화 기법 비교 등.</td><td class="explanation">향상 전략을 근거와 함께 기술해야 한다.</td></tr>
  </table>

  <h2>모의고사 4</h2>
  <table>
    <tr><th>번호</th><th>유형</th><th>정답</th><th>해설/채점 포인트</th></tr>
    <tr><td>01</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">range(1, n+1)으로 끝값을 포함하고 조건을 뒤에 적는다.</td></tr>
    <tr><td>02</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">슬라이싱은 0 기반 인덱스이므로 step-1부터 끝까지 반환한다.</td></tr>
    <tr><td>03</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">행렬곱 결과 shape는 (m, n) x (n, p) -&gt; (m, p)이다.</td></tr>
    <tr><td>04</td><td>Python 기초 / 객관식</td><td class="answer">C</td><td class="explanation">자기 내적은 각 원소 제곱의 합이다.</td></tr>
    <tr><td>05</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">arange는 stop을 포함하지 않으므로 총 원소 수는 stop/step이다.</td></tr>
    <tr><td>06</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">tuple과 str은 불변 자료형이다.</td></tr>
    <tr><td>07</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">enumerate(iterable)는 기본으로 0부터 인덱스를 부여한다.</td></tr>
    <tr><td>08</td><td>Python 기초 / 주관식</td><td class="answer">36</td><td class="explanation">등차수열 합 공식 n(n+1)/2.</td></tr>
    <tr><td>09</td><td>Python 기초 / 주관식</td><td class="answer">4</td><td class="explanation">총 원소 수를 행 수로 나누면 열 수가 된다.</td></tr>
    <tr><td>10</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">MSE는 제곱 오차 평균으로 이상치에 민감하다.</td></tr>
    <tr><td>11</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">가중치 3개와 편향 1개가 필요하다.</td></tr>
    <tr><td>12</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">발산 시에는 학습률을 낮추어 안정화한다.</td></tr>
    <tr><td>13</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">L2 패널티는 큰 가중치를 억제해 과적합을 줄인다.</td></tr>
    <tr><td>14</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관으로 계수 추정이 불안정해진다.</td></tr>
    <tr><td>15</td><td>선형 회귀 / 객관식</td><td class="answer">A</td><td class="explanation">R^2은 1에 가까울수록 좋은 설명력을 가진다.</td></tr>
    <tr><td>16</td><td>선형 회귀 / 객관식</td><td class="answer">C</td><td class="explanation">스케일을 맞추면 가중치 학습이 안정적이다.</td></tr>
    <tr><td>17</td><td>선형 회귀 / 주관식</td><td class="answer">14.4</td><td class="explanation">단일 샘플 MSE 경사는 2*(y_hat - y)*x.</td></tr>
    <tr><td>18</td><td>선형 회귀 / 주관식</td><td class="answer">0.8</td><td class="explanation">학습 비율은 1 - test_size이다.</td></tr>
    <tr><td>19</td><td>EDA / 객관식</td><td class="answer">A</td><td class="explanation">상관계수는 변수 간 선형 관계를 보여준다.</td></tr>
    <tr><td>20</td><td>EDA / 객관식</td><td class="answer">D</td><td class="explanation">박스플롯은 단일 변수 분포만 보여준다.</td></tr>
    <tr><td>21</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">bin 수는 분포 파악을 위한 설계 요소다.</td></tr>
    <tr><td>22</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">pairplot은 산점도 행렬과 변수별 분포를 제공한다.</td></tr>
    <tr><td>23</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관 변수 제거는 다중공선성을 완화한다.</td></tr>
    <tr><td>24</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">평균 대체는 분산을 줄여 데이터 변동성을 왜곡할 수 있다.</td></tr>
    <tr><td>25</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">jointplot은 산점도와 히스토그램을 결합해 보여준다.</td></tr>
    <tr><td>26</td><td>EDA / 주관식</td><td class="answer">0.032</td><td class="explanation">결측률은 missing / total로 계산한다.</td></tr>
    <tr><td>27</td><td>EDA / 주관식</td><td class="answer">1.556</td><td class="explanation">z = (x - mean) / std.</td></tr>
    <tr><td>28</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">ReLU는 양수 구간에서 기울기를 유지한다.</td></tr>
    <tr><td>29</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">완전연결층은 입력과 출력을 모두 연결하므로 가중치가 많다.</td></tr>
    <tr><td>30</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">체인룰을 사용해 기울기를 전파한다.</td></tr>
    <tr><td>31</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">softmax는 확률 분포를 반환한다.</td></tr>
    <tr><td>32</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">드롭아웃은 무작위 비활성화로 일반화 성능을 높인다.</td></tr>
    <tr><td>33</td><td>MLP / 객관식</td><td class="answer">D</td><td class="explanation">배치 정규화는 층 깊이를 줄이지 않는다.</td></tr>
    <tr><td>34</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">검증 손실이 나빠지면 학습을 중단한다.</td></tr>
    <tr><td>35</td><td>MLP / 주관식</td><td class="answer">0.2041</td><td class="explanation">He 초기화 표준편차는 sqrt(2/fan_in)이다.</td></tr>
    <tr><td>36</td><td>MLP / 주관식</td><td class="answer">180</td><td class="explanation">가중치는 input*hidden, 편향은 hidden이다.</td></tr>
    <tr><td>37</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">WordPiece는 빈도 기반 서브워드를 학습한다.</td></tr>
    <tr><td>38</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">큰 vocab은 메모리와 연산 비용을 증가시킨다.</td></tr>
    <tr><td>39</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">임베딩 차원은 표현력과 효율 사이에서 균형을 맞춘다.</td></tr>
    <tr><td>40</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">길어진 시퀀스에서 기울기가 0에 수렴한다.</td></tr>
    <tr><td>41</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">입력 게이트는 후보 상태를 얼마나 반영할지 결정한다.</td></tr>
    <tr><td>42</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">A</td><td class="explanation">패딩 토큰은 손실 계산에서 제외되도록 마스킹해야 한다.</td></tr>
    <tr><td>43</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">teacher forcing은 노출 편향을 유발할 수 있다.</td></tr>
    <tr><td>44</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">4</td><td class="explanation">필요한 패딩 수는 target_len - seq_len.</td></tr>
    <tr><td>45</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">2.25</td><td class="explanation">c_t = f_t * c_{t-1} + i_t * g_t.</td></tr>
    <tr><td>46</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 데이터 수집 및 검증 -&gt; 결측치/이상치 처리 -&gt; 특징 스케일링 -&gt; 훈련/검증 분할 -&gt; 선형 회귀 학습 -&gt; 평가 지표 보고 및 개선 방안 제시.</td><td class="explanation">각 단계의 목적과 실행 방안을 포함해야 한다.</td></tr>
    <tr><td>47</td><td>서술형 / 서술형</td><td class="answer">예시 답안: pairplot/heatmap 활용, 상관계수 기준 특징 선택, 이상치/분포 확인, 시각화별 해석.</td><td class="explanation">EDA의 목적과 도구 선택 근거를 제시해야 한다.</td></tr>
    <tr><td>48</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 입력 -&gt; 은닉층 2~3개(ReLU), 드롭아웃 0.35, 배치 정규화, 최적화 기법 선택 근거.</td><td class="explanation">모델 설계 선택을 데이터 특성과 연결해야 한다.</td></tr>
    <tr><td>49</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 서브워드 토크나이저 선택, 패딩/마스킹 처리, 임베딩 층, LSTM/GRU 구성, 정규화 및 검증 전략 제시.</td><td class="explanation">NLP 파이프라인의 단계별 설정을 구체화해야 한다.</td></tr>
    <tr><td>50</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 학습률/은닉 유닛 탐색, 교차검증, 데이터 증강, 조기 종료, 정규화 기법 비교 등.</td><td class="explanation">향상 전략을 근거와 함께 기술해야 한다.</td></tr>
  </table>

  <h2>모의고사 5</h2>
  <table>
    <tr><th>번호</th><th>유형</th><th>정답</th><th>해설/채점 포인트</th></tr>
    <tr><td>01</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">range(1, n+1)으로 끝값을 포함하고 조건을 뒤에 적는다.</td></tr>
    <tr><td>02</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">슬라이싱은 0 기반 인덱스이므로 step-1부터 끝까지 반환한다.</td></tr>
    <tr><td>03</td><td>Python 기초 / 객관식</td><td class="answer">A</td><td class="explanation">행렬곱 결과 shape는 (m, n) x (n, p) -&gt; (m, p)이다.</td></tr>
    <tr><td>04</td><td>Python 기초 / 객관식</td><td class="answer">C</td><td class="explanation">자기 내적은 각 원소 제곱의 합이다.</td></tr>
    <tr><td>05</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">arange는 stop을 포함하지 않으므로 총 원소 수는 stop/step이다.</td></tr>
    <tr><td>06</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">tuple과 str은 불변 자료형이다.</td></tr>
    <tr><td>07</td><td>Python 기초 / 객관식</td><td class="answer">B</td><td class="explanation">enumerate(iterable)는 기본으로 0부터 인덱스를 부여한다.</td></tr>
    <tr><td>08</td><td>Python 기초 / 주관식</td><td class="answer">45</td><td class="explanation">등차수열 합 공식 n(n+1)/2.</td></tr>
    <tr><td>09</td><td>Python 기초 / 주관식</td><td class="answer">4</td><td class="explanation">총 원소 수를 행 수로 나누면 열 수가 된다.</td></tr>
    <tr><td>10</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">MSE는 제곱 오차 평균으로 이상치에 민감하다.</td></tr>
    <tr><td>11</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">가중치 3개와 편향 1개가 필요하다.</td></tr>
    <tr><td>12</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">발산 시에는 학습률을 낮추어 안정화한다.</td></tr>
    <tr><td>13</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">L2 패널티는 큰 가중치를 억제해 과적합을 줄인다.</td></tr>
    <tr><td>14</td><td>선형 회귀 / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관으로 계수 추정이 불안정해진다.</td></tr>
    <tr><td>15</td><td>선형 회귀 / 객관식</td><td class="answer">A</td><td class="explanation">R^2은 1에 가까울수록 좋은 설명력을 가진다.</td></tr>
    <tr><td>16</td><td>선형 회귀 / 객관식</td><td class="answer">C</td><td class="explanation">스케일을 맞추면 가중치 학습이 안정적이다.</td></tr>
    <tr><td>17</td><td>선형 회귀 / 주관식</td><td class="answer">14.0</td><td class="explanation">단일 샘플 MSE 경사는 2*(y_hat - y)*x.</td></tr>
    <tr><td>18</td><td>선형 회귀 / 주관식</td><td class="answer">0.8</td><td class="explanation">학습 비율은 1 - test_size이다.</td></tr>
    <tr><td>19</td><td>EDA / 객관식</td><td class="answer">A</td><td class="explanation">상관계수는 변수 간 선형 관계를 보여준다.</td></tr>
    <tr><td>20</td><td>EDA / 객관식</td><td class="answer">D</td><td class="explanation">박스플롯은 단일 변수 분포만 보여준다.</td></tr>
    <tr><td>21</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">bin 수는 분포 파악을 위한 설계 요소다.</td></tr>
    <tr><td>22</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">pairplot은 산점도 행렬과 변수별 분포를 제공한다.</td></tr>
    <tr><td>23</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">높은 상관 변수 제거는 다중공선성을 완화한다.</td></tr>
    <tr><td>24</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">평균 대체는 분산을 줄여 데이터 변동성을 왜곡할 수 있다.</td></tr>
    <tr><td>25</td><td>EDA / 객관식</td><td class="answer">B</td><td class="explanation">jointplot은 산점도와 히스토그램을 결합해 보여준다.</td></tr>
    <tr><td>26</td><td>EDA / 주관식</td><td class="answer">0.033</td><td class="explanation">결측률은 missing / total로 계산한다.</td></tr>
    <tr><td>27</td><td>EDA / 주관식</td><td class="answer">1.5</td><td class="explanation">z = (x - mean) / std.</td></tr>
    <tr><td>28</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">ReLU는 양수 구간에서 기울기를 유지한다.</td></tr>
    <tr><td>29</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">완전연결층은 입력과 출력을 모두 연결하므로 가중치가 많다.</td></tr>
    <tr><td>30</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">체인룰을 사용해 기울기를 전파한다.</td></tr>
    <tr><td>31</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">softmax는 확률 분포를 반환한다.</td></tr>
    <tr><td>32</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">드롭아웃은 무작위 비활성화로 일반화 성능을 높인다.</td></tr>
    <tr><td>33</td><td>MLP / 객관식</td><td class="answer">D</td><td class="explanation">배치 정규화는 층 깊이를 줄이지 않는다.</td></tr>
    <tr><td>34</td><td>MLP / 객관식</td><td class="answer">B</td><td class="explanation">검증 손실이 나빠지면 학습을 중단한다.</td></tr>
    <tr><td>35</td><td>MLP / 주관식</td><td class="answer">0.1961</td><td class="explanation">He 초기화 표준편차는 sqrt(2/fan_in)이다.</td></tr>
    <tr><td>36</td><td>MLP / 주관식</td><td class="answer">208</td><td class="explanation">가중치는 input*hidden, 편향은 hidden이다.</td></tr>
    <tr><td>37</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">WordPiece는 빈도 기반 서브워드를 학습한다.</td></tr>
    <tr><td>38</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">큰 vocab은 메모리와 연산 비용을 증가시킨다.</td></tr>
    <tr><td>39</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">임베딩 차원은 표현력과 효율 사이에서 균형을 맞춘다.</td></tr>
    <tr><td>40</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">길어진 시퀀스에서 기울기가 0에 수렴한다.</td></tr>
    <tr><td>41</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">입력 게이트는 후보 상태를 얼마나 반영할지 결정한다.</td></tr>
    <tr><td>42</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">A</td><td class="explanation">패딩 토큰은 손실 계산에서 제외되도록 마스킹해야 한다.</td></tr>
    <tr><td>43</td><td>NLP &amp; 순환신경망 / 객관식</td><td class="answer">B</td><td class="explanation">teacher forcing은 노출 편향을 유발할 수 있다.</td></tr>
    <tr><td>44</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">4</td><td class="explanation">필요한 패딩 수는 target_len - seq_len.</td></tr>
    <tr><td>45</td><td>NLP &amp; 순환신경망 / 주관식</td><td class="answer">2.68</td><td class="explanation">c_t = f_t * c_{t-1} + i_t * g_t.</td></tr>
    <tr><td>46</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 데이터 수집 및 검증 -&gt; 결측치/이상치 처리 -&gt; 특징 스케일링 -&gt; 훈련/검증 분할 -&gt; 선형 회귀 학습 -&gt; 평가 지표 보고 및 개선 방안 제시.</td><td class="explanation">각 단계의 목적과 실행 방안을 포함해야 한다.</td></tr>
    <tr><td>47</td><td>서술형 / 서술형</td><td class="answer">예시 답안: pairplot/heatmap 활용, 상관계수 기준 특징 선택, 이상치/분포 확인, 시각화별 해석.</td><td class="explanation">EDA의 목적과 도구 선택 근거를 제시해야 한다.</td></tr>
    <tr><td>48</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 입력 -&gt; 은닉층 2~3개(ReLU), 드롭아웃 0.40, 배치 정규화, 최적화 기법 선택 근거.</td><td class="explanation">모델 설계 선택을 데이터 특성과 연결해야 한다.</td></tr>
    <tr><td>49</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 서브워드 토크나이저 선택, 패딩/마스킹 처리, 임베딩 층, LSTM/GRU 구성, 정규화 및 검증 전략 제시.</td><td class="explanation">NLP 파이프라인의 단계별 설정을 구체화해야 한다.</td></tr>
    <tr><td>50</td><td>서술형 / 서술형</td><td class="answer">예시 답안: 학습률/은닉 유닛 탐색, 교차검증, 데이터 증강, 조기 종료, 정규화 기법 비교 등.</td><td class="explanation">향상 전략을 근거와 함께 기술해야 한다.</td></tr>
  </table>
</body>
</html>
